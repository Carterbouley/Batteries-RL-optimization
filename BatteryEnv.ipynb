{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our action space is in any case discrete and each action can be assigned its integer value: \n",
    "'charge' = 0,\n",
    "'discharge' = 1,\n",
    "'wait' = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly work in **Gym** our custom environment class shall include such methods as 'init','step','reset' and 'render'. All other methods are basically helper methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common to any algorithm is that in the end certain action is choosen according to probability distribution function or stepwise function which includes two options: choice due to probability distribution or choice randomly for exploration. \n",
    "\n",
    "Anyway, the function we optimize outputs probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatteryEnv(gym.Env):\n",
    "    \"\"\"Battery optimization environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, low, high, calculate_reward_func,сheck_end_func, change_episode_idx_func, calculate_actual_signal_func, episode_idx=0, state_idx=0):\n",
    "        super(BatteryEnv, self).__init__()\n",
    "        \n",
    "        #We have only 3 discrete actions (charge,discharge,wait)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        \n",
    "        #low and high can be either numpy arrays with shape (1,n) (if we choose to have n elements \n",
    "        #in one observation vector)or can be just min and max numbers (if we choose to have only one \n",
    "        #variable for observation\n",
    "        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float16)\n",
    "        \n",
    "        # custom function to calculate reward\n",
    "        self.calculate_reward_func = calculate_reward_func\n",
    "        \n",
    "        # custom function to check whether an episode ends\n",
    "        self.сheck_end_func = сheck_end_func \n",
    "        \n",
    "        #custom function to calculate how signal has changed\n",
    "        self.calculate_actual_signal_func = calculate_actual_signal_func\n",
    "        \n",
    "        # custom function for choosing next index of episode in list of episodes (generally,we just do+=1, but sometimes it can be random)\n",
    "        self.change_episode_idx_func = change_episode_idx_func\n",
    "        \n",
    "        # index of current episode\n",
    "        self.episode_idx = episode_idx        \n",
    "        \n",
    "        # index of current state within current episode\n",
    "        self.state_idx = state_idx\n",
    "    \n",
    "    def step(self, action): \n",
    "        # here we should change input signal according to choosen action\n",
    "        # / take action according to some algorithm\n",
    "        \"\"\"\n",
    "        Method to execute one action within the environment and return reward,\n",
    "        next observation, boolean on whether episode is over and info. A time point\n",
    "        after which episode is over depends on choosen episode interval\n",
    "        which shall be a tunable hyperparameter. \n",
    "        \"\"\"\n",
    "        self.state_idx+=1 #increase state idx within episide to see what is the input_signal at t+1 in data\n",
    "        obs = self.calculate_actual_signal(self.state_idx,action) # use state idx at t+1 to see what is the new input_signal, change it with action and calculate actual signal\n",
    "        reward = self.calculate_reward_func(obs) #calculate reward from actual signal\n",
    "        done = self.сheck_end_func(self.state_idx) # check the end of episode\n",
    "        return obs, reward, done, {}\n",
    "        \n",
    "    def reset(self): \n",
    "        \"\"\"\n",
    "        here we should reset our environment and prepare it for the next episode:\n",
    "        change day/week/month counter in order for the next iteration (episode) to begin\n",
    "        \"\"\"\n",
    "        self.episode_idx = self.change_episode_idx_func() # generally we increase index of episode by 1, but we can choose other more stochastic options\n",
    "        return episodes_list[self.episode_idx]\n",
    "    \n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "    # Render the environment to the screen\n",
    "          print('random_print')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random agent test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_func(signal):\n",
    "    signal*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    #env.render()\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our calculation functions ('argument'/'hyperparameter' functions):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
